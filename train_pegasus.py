# -*- coding: utf-8 -*-
"""Train Pegasus.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TNWIbmgqmcJ-cH76kfiX25NAhiGDBy6_
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Entrenamiento"""

from transformers import PegasusTokenizer, PegasusForConditionalGeneration, Trainer, TrainingArguments
from datasets import Dataset
import torch
import os

# Desactivar Weights & Biases
os.environ["WANDB_DISABLED"] = "true"

# Configuraci√≥n
model_name = "google/pegasus-pubmed"
output_dir = "/content/drive/MyDrive/TFG/Resultados/Pegasus_entrenado_v1"
train_input_path = "/content/drive/MyDrive/TFG/Dataset/train.txt.src"
train_output_path = "/content/drive/MyDrive/TFG/Dataset/train.txt.tgt.tagged"
os.makedirs(output_dir, exist_ok=True)

# Tokenizador y modelo
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name)

# Usar solo una parte del dataset si es necesario (modo debug)
def load_dataset(input_path, output_path, max_examples=None):
    with open(input_path, "r", encoding="utf-8") as f:
        inputs = f.readlines()
    with open(output_path, "r", encoding="utf-8") as f:
        outputs = f.readlines()
    if max_examples:
        inputs = inputs[:max_examples]
        outputs = outputs[:max_examples]
    return Dataset.from_dict({
        "input": [i.strip() for i in inputs],
        "output": [o.strip() for o in outputs]
    })

# Cargar dataset (opcional: max_examples=1000)
raw_dataset = load_dataset(train_input_path, train_output_path)

# Tokenizaci√≥n eficiente
def tokenize(example):
    model_inputs = tokenizer(
        example["input"],
        padding="max_length",
        truncation=True,
        max_length=256  # input m√°s reducido
    )
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            example["output"],
            padding="max_length",
            truncation=True,
            max_length=128
        )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Preprocesar y optimizar formato
train_dataset = raw_dataset.map(tokenize, batched=True)
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# Argumentos de entrenamiento
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=1,
    num_train_epochs=1,
    learning_rate=2e-5,
    weight_decay=0.01,
    save_strategy="epoch",
    logging_dir=f"{output_dir}/logs",
    save_total_limit=1,
    remove_unused_columns=False,
    fp16=True,
    gradient_accumulation_steps=2
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset
)

# Entrenar
trainer.train()

# Guardar modelo y tokenizer
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
print(f"Entrenamiento finalizado. Modelo guardado en: {output_dir}")

"""# Validaci√≥n"""

from transformers import PegasusTokenizer, PegasusForConditionalGeneration, Trainer, TrainingArguments
from datasets import Dataset
import torch
import os
import evaluate

# Ruta al modelo entrenado
model_path = "/content/drive/MyDrive/TFG/Resultados/Pegasus_entrenado_v1"

# Rutas al dataset de validaci√≥n
val_input_path = "/content/drive/MyDrive/TFG/Dataset/val.txt.src"
val_output_path = "/content/drive/MyDrive/TFG/Dataset/val.txt.tgt.tagged"

# Cargar modelo y tokenizer
tokenizer = PegasusTokenizer.from_pretrained(model_path)
model = PegasusForConditionalGeneration.from_pretrained(model_path)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Leer test
with open(val_input_path, "r", encoding="utf-8") as f:
    test_inputs = [line.strip() for line in f.readlines()]
with open(val_output_path, "r", encoding="utf-8") as f:
    test_refs = [line.strip() for line in f.readlines()]

assert len(test_inputs) == len(test_refs), "Longitudes no coinciden"

# Generar
generated_summaries = []
model.eval()
with torch.no_grad():
    for text in tqdm(test_inputs):
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding="longest",
            max_length=512
        ).to(model.device)

        summary_ids = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=128,
            num_beams=4,
            early_stopping=True
        )

        decoded = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        generated_summaries.append(decoded)

# ROUGE
rouge = evaluate.load("rouge")
results = rouge.compute(
    predictions=generated_summaries,
    references=test_refs,
    use_stemmer=True
)
results = {k.upper(): round(v * 100, 2) for k, v in results.items()}

print("\nüìä M√âTRICAS ROUGE ‚Äì VAL:")
for key, value in results.items():
    print(f"{key}: {value}")

"""# Test"""

from transformers import PegasusTokenizer, PegasusForConditionalGeneration, Trainer, TrainingArguments
from datasets import Dataset
import torch
import os
import evaluate

# Ruta al modelo entrenado
model_path = "/content/drive/MyDrive/TFG/Resultados/Pegasus_entrenado_v1"

# Rutas al dataset de validaci√≥n
test_input_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.src"
test_output_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.tgt.tagged"

# Cargar modelo y tokenizer
tokenizer = PegasusTokenizer.from_pretrained(model_path)
model = PegasusForConditionalGeneration.from_pretrained(model_path)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Leer test
with open(test_input_path, "r", encoding="utf-8") as f:
    test_inputs = [line.strip() for line in f.readlines()]
with open(test_output_path, "r", encoding="utf-8") as f:
    test_refs = [line.strip() for line in f.readlines()]

assert len(test_inputs) == len(test_refs), "‚ùå Longitudes no coinciden"

# Generar
generated_summaries = []
model.eval()
with torch.no_grad():
    for text in tqdm(test_inputs):
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding="longest",
            max_length=512
        ).to(model.device)

        summary_ids = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=128,
            num_beams=4,
            early_stopping=True
        )

        decoded = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        generated_summaries.append(decoded)

# ROUGE
rouge = evaluate.load("rouge")
results = rouge.compute(
    predictions=generated_summaries,
    references=test_refs,
    use_stemmer=True
)
results = {k.upper(): round(v * 100, 2) for k, v in results.items()}

print("\nüìä M√âTRICAS ROUGE ‚Äì TEST:")
for key, value in results.items():
    print(f"{key}: {value}")