# -*- coding: utf-8 -*-
"""Train T5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17YQGlNXx2f9Yzo0Ifn2fntj8b6gbSvQC
"""

from google.colab import drive
drive.mount('/content/drive')

"""# 1r entrenamiento"""

from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments
from datasets import Dataset
import os

# Rutas
train_input_path = "/content/drive/MyDrive/TFG/Dataset/train.txt.src"
train_output_path = "/content/drive/MyDrive/TFG/Dataset/train.txt.tgt.tagged"
output_dir = "/content/drive/MyDrive/TFG/Resultados/T5_entrenado_v1"
os.makedirs(output_dir, exist_ok=True)

# Cargar modelo base y tokenizer
model = T5ForConditionalGeneration.from_pretrained("t5-base")
tokenizer = T5Tokenizer.from_pretrained("t5-base")

# Cargar y preparar dataset
def load_dataset(input_path, output_path):
    with open(input_path, "r", encoding="utf-8") as f:
        inputs = f.readlines()
    with open(output_path, "r", encoding="utf-8") as f:
        outputs = f.readlines()
    assert len(inputs) == len(outputs), "Input/Output no coinciden en longitud"
    return Dataset.from_dict({"input": [i.strip() for i in inputs], "output": [o.strip() for o in outputs]})

raw_dataset = load_dataset(train_input_path, train_output_path)

# Tokenizaci√≥n con max_length diferenciado
def tokenize(example):
    model_inputs = tokenizer(example["input"], padding="max_length", truncation=True, max_length=192)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(example["output"], padding="max_length", truncation=True, max_length=512)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

train_dataset = raw_dataset.map(tokenize, batched=True)
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# Configuraci√≥n de entrenamiento
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=1,
    num_train_epochs=2,
    learning_rate=2e-5,
    weight_decay=0.01,
    save_strategy="no",
    logging_dir=f"{output_dir}/logs",
    remove_unused_columns=False,
)

# Entrenador
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

# Entrenamiento
trainer.train()

# Guardado final
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
print(f"Entrenamiento finalizado. Modelo guardado en: {output_dir}")

"""# 1a validaci√≥n"""

from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch
import evaluate

# Ruta al modelo entrenado
model_path = "/content/drive/MyDrive/TFG/Resultados/T5_entrenado_v1"

# Rutas al dataset de validaci√≥n
val_input_path = "/content/drive/MyDrive/TFG/Dataset/val.txt.src"
val_output_path = "/content/drive/MyDrive/TFG/Dataset/val.txt.tgt.tagged"

# Cargar modelo y tokenizer
model = T5ForConditionalGeneration.from_pretrained(model_path)
tokenizer = T5Tokenizer.from_pretrained(model_path)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Leer ejemplos
with open(val_input_path, "r", encoding="utf-8") as f:
    val_inputs = [line.strip() for line in f.readlines()]
with open(val_output_path, "r", encoding="utf-8") as f:
    val_targets = [line.strip() for line in f.readlines()]

# Usamos solo 300 ejemplos para la validaci√≥n r√°pida
val_inputs = val_inputs[:300]
val_targets = val_targets[:300]

# Generar predicciones
generated_outputs = []
for text in val_inputs:
    input_ids = tokenizer(text, return_tensors="pt", truncation=True, max_length=192).input_ids.to(device)
    output_ids = model.generate(input_ids, max_length=512, num_beams=4)
    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    generated_outputs.append(decoded)

# Evaluaci√≥n con ROUGE
rouge = evaluate.load("rouge")
results = rouge.compute(
    predictions=generated_outputs,
    references=val_targets,
    use_stemmer=True
)
results = {k: round(v * 100, 2) for k, v in results.items()}

# Mostrar resultados
print("M√âTRICAS ROUGE ‚Äì VALIDACI√ìN (val.txt ‚Äì 300 ejemplos):")
for k, v in results.items():
    print(f"{k.upper()}: {v}")

"""# 2o entrenamiento"""

from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments
from datasets import Dataset
import os

# Rutas
model_path = "/content/drive/MyDrive/TFG/Resultados/T5_entrenado_v1"
output_dir = "/content/drive/MyDrive/TFG/Resultados/T5_entrenado_v2"
os.makedirs(output_dir, exist_ok=True)

# Cargar modelo entrenado previamente
model = T5ForConditionalGeneration.from_pretrained(model_path)
tokenizer = T5Tokenizer.from_pretrained(model_path)

# üìÑ Dataset (input = .src / output = .tgt.tagged)
train_input = "/content/drive/MyDrive/TFG/Dataset/train.txt.src"
train_output = "/content/drive/MyDrive/TFG/Dataset/train.txt.tgt.tagged"

# Cargar dataset
def load_dataset(input_path, output_path):
    with open(input_path, "r", encoding="utf-8") as f:
        inputs = f.readlines()
    with open(output_path, "r", encoding="utf-8") as f:
        outputs = f.readlines()
    assert len(inputs) == len(outputs), "‚ö†Los archivos no tienen la misma cantidad de l√≠neas"
    return Dataset.from_dict({"input": [i.strip() for i in inputs], "output": [o.strip() for o in outputs]})

# Tokenizaci√≥n con longitudes optimizadas
def tokenize(example):
    return tokenizer(
        example["input"],
        text_target=example["output"],
        padding="max_length",
        truncation=True,
        max_length=192  # para INPUT
    )

train_dataset = load_dataset(train_input, train_output)
train_dataset = train_dataset.map(tokenize, batched=True)
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# Par√°metros de entrenamiento (1 √©poca m√°s)
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=1,
    num_train_epochs=1,
    learning_rate=2e-5,
    weight_decay=0.01,
    save_strategy="epoch",
    logging_dir=f"{output_dir}/logs",
    remove_unused_columns=False,
    save_total_limit=2
)

# Entrenador
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset
)

# Reentrenamiento
trainer.train()

# Guardar modelo
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"Reentrenamiento completado. Modelo guardado en:\n{output_dir}")

"""# 2a validaci√≥n"""

from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch
import evaluate

# Ruta al modelo entrenado
model_path = "/content/drive/MyDrive/TFG/Resultados/T5_entrenado_v2"

# Rutas al dataset de validaci√≥n
val_input_path = "/content/drive/MyDrive/TFG/Dataset/val.txt.src"
val_output_path = "/content/drive/MyDrive/TFG/Dataset/val.txt.tgt.tagged"

# Cargar modelo y tokenizer
model = T5ForConditionalGeneration.from_pretrained(model_path)
tokenizer = T5Tokenizer.from_pretrained(model_path)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Leer ejemplos
with open(val_input_path, "r", encoding="utf-8") as f:
    val_inputs = [line.strip() for line in f.readlines()]
with open(val_output_path, "r", encoding="utf-8") as f:
    val_targets = [line.strip() for line in f.readlines()]

# Usamos solo 300 ejemplos para la validaci√≥n r√°pida
val_inputs = val_inputs[:300]
val_targets = val_targets[:300]

# Generar predicciones
generated_outputs = []
for text in val_inputs:
    input_ids = tokenizer(text, return_tensors="pt", truncation=True, max_length=192).input_ids.to(device)
    output_ids = model.generate(input_ids, max_length=512, num_beams=4)
    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    generated_outputs.append(decoded)

# Evaluaci√≥n con ROUGE
rouge = evaluate.load("rouge")
results = rouge.compute(
    predictions=generated_outputs,
    references=val_targets,
    use_stemmer=True
)
results = {k: round(v * 100, 2) for k, v in results.items()}

# Mostrar resultados
print("M√âTRICAS ROUGE ‚Äì VALIDACI√ìN (val.txt ‚Äì 300 ejemplos):")
for k, v in results.items():
    print(f"{k.upper()}: {v}")

"""# 3r entrenamiento"""

from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments
from datasets import Dataset
import os

# Rutas del modelo anterior y de salida
model_path = "/content/drive/MyDrive/TFG/Resultados/T5_entrenado_v2"
output_dir = "/content/drive/MyDrive/TFG/Resultados/T5_entrenado_v3"
os.makedirs(output_dir, exist_ok=True)

# Cargar modelo y tokenizer previamente entrenados
model = T5ForConditionalGeneration.from_pretrained(model_path)
tokenizer = T5Tokenizer.from_pretrained(model_path)

# Dataset (input = t√≠tulo + abstract, output = resumen)
train_input = "/content/drive/MyDrive/TFG/Dataset/train.txt.src"
train_output = "/content/drive/MyDrive/TFG/Dataset/train.txt.tgt.tagged"

# Cargar el dataset desde los archivos
def load_dataset(input_path, output_path):
    with open(input_path, "r", encoding="utf-8") as f:
        inputs = f.readlines()
    with open(output_path, "r", encoding="utf-8") as f:
        outputs = f.readlines()
    assert len(inputs) == len(outputs), "‚ùå Inputs y outputs no coinciden en tama√±o"
    return Dataset.from_dict({
        "input": [i.strip() for i in inputs],
        "output": [o.strip() for o in outputs]
    })

# Tokenizaci√≥n con longitud adecuada
def tokenize(example):
    return tokenizer(
        example["input"],
        text_target=example["output"],
        padding="max_length",
        truncation=True,
        max_length=192
    )

# Preprocesamiento
train_dataset = load_dataset(train_input, train_output)
train_dataset = train_dataset.map(tokenize, batched=True)
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# Argumentos de entrenamiento (1 √∫ltima √©poca)
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=1,
    num_train_epochs=1,
    learning_rate=2e-5,
    weight_decay=0.01,
    save_strategy="no",
    logging_dir=f"{output_dir}/logs",
    remove_unused_columns=False,
    save_total_limit=2
)

# Entrenador
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset
)

# Entrenamiento
trainer.train()

# Guardado final
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"Entrenamiento FINAL completado. Modelo v3 guardado en:\n{output_dir}")

pip install rouge_score

"""# 3a validaci√≥n"""

from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch
import evaluate

# Ruta al modelo entrenado
model_path = "/content/drive/MyDrive/TFG/Resultados/T5_entrenado_v3"

# Rutas al dataset de validaci√≥n
val_input_path = "/content/drive/MyDrive/TFG/Dataset/val.txt.src"
val_output_path = "/content/drive/MyDrive/TFG/Dataset/val.txt.tgt.tagged"

# Cargar modelo y tokenizer
model = T5ForConditionalGeneration.from_pretrained(model_path, local_files_only=True)
tokenizer = T5Tokenizer.from_pretrained(model_path, local_files_only=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Leer ejemplos
with open(val_input_path, "r", encoding="utf-8") as f:
    val_inputs = [line.strip() for line in f.readlines()]
with open(val_output_path, "r", encoding="utf-8") as f:
    val_targets = [line.strip() for line in f.readlines()]

# Usamos solo 300 ejemplos para la validaci√≥n r√°pida
val_inputs = val_inputs[:300]
val_targets = val_targets[:300]

# Generar predicciones
generated_outputs = []
for text in val_inputs:
    input_ids = tokenizer(text, return_tensors="pt", truncation=True, max_length=192).input_ids.to(device)
    output_ids = model.generate(input_ids, max_length=512, num_beams=4)
    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    generated_outputs.append(decoded)

# Evaluaci√≥n con ROUGE
rouge = evaluate.load("rouge")
results = rouge.compute(
    predictions=generated_outputs,
    references=val_targets,
    use_stemmer=True
)
results = {k: round(v * 100, 2) for k, v in results.items()}

# Mostrar resultados
print("M√âTRICAS ROUGE ‚Äì VALIDACI√ìN (val.txt ‚Äì 300 ejemplos):")
for k, v in results.items():
    print(f"{k.upper()}: {v}")

"""# Test"""

from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch
import evaluate

# Ruta del modelo entrenado
model_path = "/content/drive/MyDrive/TFG/Resultados/T5_entrenado_v3"

# Rutas al dataset de test
test_input_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.src"
test_output_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.tgt.tagged"

# Cargar modelo y tokenizer
model = T5ForConditionalGeneration.from_pretrained(model_path)
tokenizer = T5Tokenizer.from_pretrained(model_path)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Cargar datos
with open(test_input_path, "r", encoding="utf-8") as f:
    test_inputs = [line.strip() for line in f.readlines()]
with open(test_output_path, "r", encoding="utf-8") as f:
    test_targets = [line.strip() for line in f.readlines()]

# Limitar a 300 ejemplos para evitar problemas de RAM
test_inputs = test_inputs[:300]
test_targets = test_targets[:300]

# Generar predicciones
generated_outputs = []
for text in test_inputs:
    input_ids = tokenizer(text, return_tensors="pt", truncation=True, max_length=192).input_ids.to(device)
    output_ids = model.generate(input_ids, max_length=512, num_beams=4)
    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    generated_outputs.append(decoded)

# Evaluaci√≥n ROUGE
rouge = evaluate.load("rouge")
results = rouge.compute(
    predictions=generated_outputs,
    references=test_targets,
    use_stemmer=True
)
results = {k: round(v * 100, 2) for k, v in results.items()}

# Mostrar resultados
print("M√âTRICAS ROUGE ‚Äì TEST (test.txt ‚Äì 300 ejemplos):")
for k, v in results.items():
    print(f"{k.upper()}: {v}")
