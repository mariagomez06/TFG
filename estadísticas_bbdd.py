# -*- coding: utf-8 -*-
"""Estadísticas BBDD

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jkPY4eALtqtt9Lz_fc5G-TAcPO1Qk89h
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Ejemplos BBDD"""

# Definir las rutas a los archivos de entrada (src) y salida (tgt)
dataset_paths = {
    "train": {
        "src": "/content/drive/MyDrive/TFG/Dataset/train.txt.src",
        "tgt": "/content/drive/MyDrive/TFG/Dataset/train.txt.tgt.tagged"
    },
    "val": {
        "src": "/content/drive/MyDrive/TFG/Dataset/val.txt.src",
        "tgt": "/content/drive/MyDrive/TFG/Dataset/val.txt.tgt.tagged"
    },
    "test": {
        "src": "/content/drive/MyDrive/TFG/Dataset/test.txt.src",
        "tgt": "/content/drive/MyDrive/TFG/Dataset/test.txt.tgt.tagged"
    }
}

# Función para leer ejemplos de cada conjunto
def mostrar_ejemplos(dataset_paths, num_ejemplos=2):
    for split, paths in dataset_paths.items():
        print(f"\n--- {split.upper()} SET ---\n")
        with open(paths["src"], encoding="utf-8") as f_src, open(paths["tgt"], encoding="utf-8") as f_tgt:
            src_lines = f_src.readlines()
            tgt_lines = f_tgt.readlines()

            for i in range(min(num_ejemplos, len(src_lines))):
                print(f">>> Ejemplo {i+1}")
                print("Entrada (src):", src_lines[i].strip())
                print("Salida esperada (tgt):", tgt_lines[i].strip())
                print("-" * 40)

# Ejecutar la función para mostrar los ejemplos
mostrar_ejemplos(dataset_paths, num_ejemplos=2)

"""# Train"""

# Cargar input/output
def load_dataset(input_path, output_path):
    with open(input_path, "r", encoding="utf-8") as f:
        inputs = [line.strip() for line in f.readlines()]
    with open(output_path, "r", encoding="utf-8") as f:
        outputs = [line.strip() for line in f.readlines()]

    assert len(inputs) == len(outputs), "❌ Input y output tienen diferente número de líneas"
    return inputs, outputs

output_path = "/content/drive/MyDrive/TFG/Dataset/train.txt.tgt.tagged"
input_path = "/content/drive/MyDrive/TFG/Dataset/train.txt.src"

inputs, outputs = load_dataset(input_path, output_path)
print(f" Dataset cargado con {len(inputs)} ejemplos.")

#Estadísticas básicas de longitud
import numpy as np

# Función para contar palabras
def word_count(text):
    return len(text.split())

# Calcular longitudes
input_lengths = [word_count(text) for text in inputs]
output_lengths = [word_count(text) for text in outputs]

# Estadísticas
print("📊 MÉTRICAS DEL DATASET:\n")

print(f"Número total de ejemplos: {len(inputs)}")

print("\nLongitud del input (texto de entrada):")
print(f"   ▪ Media: {np.mean(input_lengths):.2f} palabras")
print(f"   ▪ Máxima: {np.max(input_lengths)} palabras")
print(f"   ▪ Mínima: {np.min(input_lengths)} palabras")

print("\nLongitud del output (texto generado):")
print(f"   ▪ Media: {np.mean(output_lengths):.2f} palabras")
print(f"   ▪ Máxima: {np.max(output_lengths)} palabras")
print(f"   ▪ Mínima: {np.min(output_lengths)} palabras")

import matplotlib.pyplot as plt

# Filtro para textos con ≤ 300 palabras
filtered_input_lengths = [l for l in input_lengths if l <= 300]
filtered_output_lengths = [l for l in output_lengths if l <= 300]

# Gráfico
plt.style.use("seaborn-v0_8-muted")
plt.figure(figsize=(12, 5))

plt.hist(filtered_input_lengths, bins=50, alpha=0.7, label="Input (entrada)", color="skyblue")
plt.hist(filtered_output_lengths, bins=50, alpha=0.7, label="Output (salida)", color="lightgreen")
plt.xlabel("Número de palabras (máx. 300)")
plt.ylabel("Frecuencia")
plt.title("Distribución de longitud de textos (train - input vs output, ≤ 300 palabras)")
plt.legend()
plt.grid(True)
plt.show()

"""# Validation"""

# Cargar val.txt
val_output_path = "/content/drive/MyDrive/TFG/Dataset/val.txt.tgt.tagged"
val_input_path = "/content/drive/MyDrive/TFG/Dataset/val.txt.src"

def load_dataset(input_path, output_path):
    with open(input_path, "r", encoding="utf-8") as f:
        inputs = [line.strip() for line in f.readlines()]
    with open(output_path, "r", encoding="utf-8") as f:
        outputs = [line.strip() for line in f.readlines()]

    assert len(inputs) == len(outputs), "Input y output tienen diferente número de líneas"
    return inputs, outputs

val_inputs, val_outputs = load_dataset(val_input_path, val_output_path)
print(f"Dataset de validación cargado con {len(val_inputs)} ejemplos.")

import numpy as np

# Función para contar palabras
def word_count(text):
    return len(text.split())

# Calcular longitudes
val_input_lengths = [word_count(text) for text in val_inputs]
val_output_lengths = [word_count(text) for text in val_outputs]

# Estadísticas
print("📊 MÉTRICAS DEL CONJUNTO DE VALIDACIÓN:\n")

print(f"🔢 Número total de ejemplos: {len(val_inputs)}")

print("\n🟦 Longitud del input (texto de entrada):")
print(f"   ▪ Media: {np.mean(val_input_lengths):.2f} palabras")
print(f"   ▪ Máxima: {np.max(val_input_lengths)} palabras")
print(f"   ▪ Mínima: {np.min(val_input_lengths)} palabras")

print("\n🟩 Longitud del output (texto objetivo):")
print(f"   ▪ Media: {np.mean(val_output_lengths):.2f} palabras")
print(f"   ▪ Máxima: {np.max(val_output_lengths)} palabras")
print(f"   ▪ Mínima: {np.min(val_output_lengths)} palabras")

import matplotlib.pyplot as plt

# Filtro para textos con ≤ 300 palabras
filtered_input_lengths = [l for l in val_input_lengths if l <= 300]
filtered_output_lengths = [l for l in val_output_lengths if l <= 300]

# Gráfico
plt.style.use("seaborn-v0_8-muted")
plt.figure(figsize=(12, 5))

plt.hist(filtered_input_lengths, bins=50, alpha=0.7, label="Input (entrada)", color="skyblue")
plt.hist(filtered_output_lengths, bins=50, alpha=0.7, label="Output (salida)", color="lightgreen")
plt.xlabel("Número de palabras (máx. 300)")
plt.ylabel("Frecuencia")
plt.title("Distribución de longitud de textos (val - input vs output, ≤ 300 palabras)")
plt.legend()
plt.grid(True)
plt.show()

"""# Test"""

# Rutas del conjunto de test
test_output_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.tgt.tagged"
test_input_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.src"

# Función de carga
def load_dataset(input_path, output_path):
    with open(input_path, "r", encoding="utf-8") as f:
        inputs = [line.strip() for line in f.readlines()]
    with open(output_path, "r", encoding="utf-8") as f:
        outputs = [line.strip() for line in f.readlines()]

    assert len(inputs) == len(outputs), "Input y output tienen diferente número de líneas"
    return inputs, outputs

# Cargar datos
test_inputs, test_outputs = load_dataset(test_input_path, test_output_path)
print(f"Dataset de TEST cargado con {len(test_inputs)} ejemplos.")

import numpy as np

# Función para contar palabras
def word_count(text):
    return len(text.split())

# Calcular longitudes
test_input_lengths = [word_count(text) for text in test_inputs]
test_output_lengths = [word_count(text) for text in test_outputs]

# Estadísticas
print("MÉTRICAS DEL CONJUNTO DE TEST:\n")

print(f"Número total de ejemplos: {len(test_inputs)}")

print("\nLongitud del input (texto de entrada):")
print(f"   ▪ Media: {np.mean(test_input_lengths):.2f} palabras")
print(f"   ▪ Máxima: {np.max(test_input_lengths)} palabras")
print(f"   ▪ Mínima: {np.min(test_input_lengths)} palabras")

print("\nLongitud del output (texto objetivo):")
print(f"   ▪ Media: {np.mean(test_output_lengths):.2f} palabras")
print(f"   ▪ Máxima: {np.max(test_output_lengths)} palabras")
print(f"   ▪ Mínima: {np.min(test_output_lengths)} palabras")

import matplotlib.pyplot as plt

# Filtro para ≤ 300 palabras
filtered_test_input_lengths = [l for l in test_input_lengths if l <= 300]
filtered_test_output_lengths = [l for l in test_output_lengths if l <= 300]

# Gráfico
plt.style.use("seaborn-v0_8-muted")
plt.figure(figsize=(12, 5))

plt.hist(filtered_test_input_lengths, bins=50, alpha=0.7, label="Input (entrada)", color="skyblue")
plt.hist(filtered_test_output_lengths, bins=50, alpha=0.7, label="Output (salida)", color="lightgreen")
plt.xlabel("Número de palabras (máx. 300)")
plt.ylabel("Frecuencia")
plt.title("Distribución de longitud de textos (test – input vs output, ≤ 300 palabras)")
plt.legend()
plt.grid(True)
plt.show()

"""# max_length - Análisis de truncado"""

# Función para estimar cuántos ejemplos serían truncados
def truncation_analysis(lengths, name, thresholds=[64, 128, 192, 256, 512]):
    print(f"\nAnálisis de truncamiento para: {name}")
    total = len(lengths)

    for limit in thresholds:
        truncated = sum(1 for l in lengths if l > limit)
        percentage = (truncated / total) * 100
        print(f" - >{limit} palabras: {truncated} ejemplos ({percentage:.2f}%)")

# Ejecutar sobre los 3 datasets si están cargados

# TRAIN
truncation_analysis(input_lengths, "TRAIN input")
truncation_analysis(output_lengths, "TRAIN output")

# VAL
truncation_analysis(val_input_lengths, "VAL input")
truncation_analysis(val_output_lengths, "VAL output")

# TEST
truncation_analysis(test_input_lengths, "TEST input")
truncation_analysis(test_output_lengths, "TEST output")

import matplotlib.pyplot as plt

def plot_truncation_curves(length_lists, labels, title):
    thresholds = list(range(64, 513, 32))  # de 64 a 512 cada 32
    plt.figure(figsize=(10, 6))
    plt.style.use("seaborn-v0_8-muted")

    for lengths, label in zip(length_lists, labels):
        trunc_percents = []
        total = len(lengths)
        for limit in thresholds:
            truncated = sum(1 for l in lengths if l > limit)
            percentage = (truncated / total) * 100
            trunc_percents.append(percentage)

        plt.plot(thresholds, trunc_percents, label=label)

    plt.xlabel("max_length (palabras)")
    plt.ylabel("% de ejemplos truncados")
    plt.title(title)
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

# Entradas
plot_truncation_curves(
    [input_lengths, val_input_lengths, test_input_lengths],
    ["TRAIN input", "VAL input", "TEST input"],
    "Porcentaje de truncamiento según max_length (INPUT)"
)

# Salidas
plot_truncation_curves(
    [output_lengths, val_output_lengths, test_output_lengths],
    ["TRAIN output", "VAL output", "TEST output"],
    "Porcentaje de truncamiento según max_length (OUTPUT)"
)

# Reimportar paquetes tras el reset del entorno
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pathlib import Path

# Rutas de los archivos
train_output_path = "/content/drive/MyDrive/TFG/Dataset/train.txt.tgt.tagged"
train_input_path = "/content/drive/MyDrive/TFG/Dataset/train.txt.src"
val_output_path = "/content/drive/MyDrive/TFG/Dataset/val.txt.tgt.tagged"
val_input_path = "/content/drive/MyDrive/TFG/Dataset/val.txt.src"
test_output_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.tgt.tagged"
test_input_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.src"

# Leer archivos
train_inputs = Path(train_input_path).read_text(encoding="utf-8").splitlines()
train_outputs = Path(train_output_path).read_text(encoding="utf-8").splitlines()
val_inputs = Path(val_input_path).read_text(encoding="utf-8").splitlines()
val_outputs = Path(val_output_path).read_text(encoding="utf-8").splitlines()
test_inputs = Path(test_input_path).read_text(encoding="utf-8").splitlines()
test_outputs = Path(test_output_path).read_text(encoding="utf-8").splitlines()

# Crear DataFrames para cada conjunto
def build_length_df(texts, split, kind):
    return pd.DataFrame({
        "length": [len(x.split()) for x in texts],
        "split": split,
        "type": kind
    })

df = pd.concat([
    build_length_df(train_inputs, "train", "input"),
    build_length_df(train_outputs, "train", "output"),
    build_length_df(val_inputs, "val", "input"),
    build_length_df(val_outputs, "val", "output"),
    build_length_df(test_inputs, "test", "input"),
    build_length_df(test_outputs, "test", "output"),
], ignore_index=True)

# Crear gráfico
plt.figure(figsize=(14, 6))
sns.boxplot(data=df, x="split", y="length", hue="type", palette="Set2")
plt.title("Distribución de longitudes (palabras) por conjunto y tipo")
plt.ylabel("Número de palabras")
plt.xlabel("Conjunto de datos")
plt.legend(title="Tipo de texto")
plt.tight_layout()
plt.show()

