# -*- coding: utf-8 -*-
"""Test MultiLevelRichCorpus

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Aev4-6fOyXRhRczpS_9X65Tu4JC8UYG8
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Creaci√≥n de listas input y output"""

import pandas as pd
import os

# Ruta al archivo original (aj√∫stala si est√°s en Drive)
csv_path = '/content/drive/My Drive/TFG/MultiLevelRichCorpus/MultiLevelRichCorpus.csv'
output_dir = "/content/drive/MyDrive/TFG/MultiLevelRichCorpus"

# Leer el CSV (saltando filas vac√≠as si las hay)
df = pd.read_csv(csv_path, skiprows=2, encoding='utf-8')

# Eliminar columnas innecesarias
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df.columns = df.columns.str.strip().str.lower()

# Verificar que existen las columnas necesarias
required_columns = {'titulo', 'abstract', 'state of the art'}
if not required_columns.issubset(df.columns):
    raise ValueError(f"Faltan columnas requeridas: {required_columns - set(df.columns)}")

# Eliminar filas con campos vac√≠os
df = df.dropna(subset=['titulo', 'abstract', 'state of the art'])

# Generar input y output
df['input'] = df['titulo'].fillna('') + ' ' + df['abstract'].fillna('')
inputs = df['input'].tolist()
outputs = df['state of the art'].tolist()

# Guardar en archivos alineados
with open(os.path.join(output_dir, "data.src"), "w", encoding="utf-8") as f:
    for line in inputs:
        f.write(line.strip() + '\n')

with open(os.path.join(output_dir, "data.tagged.src"), "w", encoding="utf-8") as f:
    for line in outputs:
        f.write(line.strip() + '\n')

print("‚úÖ Archivos guardados correctamente:")
print("- data.src (titulo + abstract)")
print("- data.tagged.src (state of the art)")

# Mostrar ejemplos de input/output alineados
print("\nEJEMPLOS DE ENTRADAS Y SALIDAS\n" + "-"*40)
for i in range(min(5, len(inputs))):  # Muestra hasta 5 ejemplos
    print(f"Ejemplo {i+1}:")
    print("INPUT:")
    print(inputs[i])
    print("OUTPUT:")
    print(outputs[i])
    print("-" * 40)

"""# Uso modelo T5 y Rouge"""

from transformers import T5Tokenizer, T5ForConditionalGeneration
from datasets import load_metric
from rouge_score import rouge_scorer
import torch
import pandas as pd

# Rutas locales o en Google Drive
model_path = '/content/drive/MyDrive/TFG/Resultados/T5_entrenado_v3'
input_path = '/content/drive/MyDrive/TFG/MultiLevelRichCorpus/data.sep.src'
reference_path = '/content/drive/MyDrive/TFG/MultiLevelRichCorpus/data.sep.tagged.src'

# Cargar modelo y tokenizer
tokenizer = T5Tokenizer.from_pretrained(model_path)
model = T5ForConditionalGeneration.from_pretrained(model_path).to("cuda" if torch.cuda.is_available() else "cpu")
device = model.device

# Leer y separar los ejemplos
def load_sep_file(path):
    with open(path, 'r', encoding='utf-8') as f:
        content = f.read()
    return [ex.strip() for ex in content.split("<SEP>") if ex.strip()]

input_examples = load_sep_file(input_path)
reference_summaries = [ex.replace('$', '') for ex in load_sep_file(reference_path)]

assert len(input_examples) == len(reference_summaries), "N√∫mero de inputs y outputs no coincide"

# Generar res√∫menes
generated_summaries = []
for text in input_examples:
    input_ids = tokenizer.encode(text, return_tensors="pt", truncation=True, max_length=512).to(device)
    output_ids = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)
    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    generated_summaries.append(decoded)

# M√©trica ROUGE global
rouge = load_metric("rouge")
results = rouge.compute(predictions=generated_summaries, references=reference_summaries)

print("\nM√âTRICAS ROUGE ‚Äì Evaluaci√≥n global:")
for key in results:
    print(f"{key.upper()}: {results[key].mid.fmeasure * 100:.2f}")

# ROUGE-L individual
scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
scores = [scorer.score(ref, pred)['rougeL'].fmeasure for ref, pred in zip(reference_summaries, generated_summaries)]

# Mostrar mejores y peores ejemplos
df = pd.DataFrame({
    'input': input_examples,
    'reference': reference_summaries,
    'generated': generated_summaries,
    'rougeL': scores
}).sort_values(by='rougeL', ascending=False)

def print_examples(df_subset, title):
    print(f"\n{'='*20} {title} {'='*20}")
    for _, row in df_subset.iterrows():
        print(f"\nInput:\n{row['input']}\n")
        print(f"Referencia:\n{row['reference']}\n")
        print(f"Generado:\n{row['generated']}\n")
        print(f"ROUGE-L: {row['rougeL']:.4f}")
        print('-' * 60)

print_examples(df.head(3), "MEJORES EJEMPLOS")
print_examples(df.tail(3), "PEORES EJEMPLOS")

"""# Pegasus"""

from transformers import PegasusTokenizer, PegasusForConditionalGeneration
from datasets import load_metric
from rouge_score import rouge_scorer
import torch
import pandas as pd

# Rutas en tu Drive
model_path = '/content/drive/MyDrive/TFG/Resultados/Pegasus_entrenado_v1'
input_path = '/content/drive/MyDrive/TFG/MultiLevelRichCorpus/data.sep.src'
reference_path = '/content/drive/MyDrive/TFG/MultiLevelRichCorpus/data.sep.tagged.src'

# Cargar modelo y tokenizer
tokenizer = PegasusTokenizer.from_pretrained(model_path)
model = PegasusForConditionalGeneration.from_pretrained(model_path).to("cuda" if torch.cuda.is_available() else "cpu")
device = model.device

# Leer y separar los ejemplos
def load_sep_file(path):
    with open(path, 'r', encoding='utf-8') as f:
        content = f.read()
    return [ex.strip() for ex in content.split("<SEP>") if ex.strip()]

input_examples = load_sep_file(input_path)
reference_summaries = [ex.replace('$', '') for ex in load_sep_file(reference_path)]

assert len(input_examples) == len(reference_summaries), "N√∫mero de inputs y outputs no coincide"

# Generar res√∫menes
generated_summaries = []
for text in input_examples:
    inputs = tokenizer(text, truncation=True, padding="longest", return_tensors="pt").to(device)
    summary_ids = model.generate(**inputs, max_length=150, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    generated_summaries.append(summary)

# M√©trica ROUGE global
rouge = load_metric("rouge")
results = rouge.compute(predictions=generated_summaries, references=reference_summaries)

print("\nüìä M√âTRICAS ROUGE ‚Äì Evaluaci√≥n global:")
for key in results:
    print(f"{key.upper()}: {results[key].mid.fmeasure * 100:.2f}")

# ROUGE-L individual
scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
scores = [scorer.score(ref, pred)['rougeL'].fmeasure for ref, pred in zip(reference_summaries, generated_summaries)]

# Mostrar mejores y peores ejemplos
df = pd.DataFrame({
    'input': input_examples,
    'reference': reference_summaries,
    'generated': generated_summaries,
    'rougeL': scores
}).sort_values(by='rougeL', ascending=False)

def print_examples(df_subset, title):
    print(f"\n{'='*20} {title} {'='*20}")
    for _, row in df_subset.iterrows():
        print(f"\nInput:\n{row['input']}\n")
        print(f"Referencia:\n{row['reference']}\n")
        print(f"Generado:\n{row['generated']}\n")
        print(f"ROUGE-L: {row['rougeL']:.4f}")
        print('-' * 60)

print_examples(df.head(3), "MEJORES EJEMPLOS")
print_examples(df.tail(3), "PEORES EJEMPLOS")