# -*- coding: utf-8 -*-
"""Modelos sin entrenar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q2iBg_S_8fjJXlr1wWQJ57j75l6PZLFv

# t5-base

Modelo multitarea de texto a texto (Text-to-Text Transfer Transformer) con 220M par√°metros. Requiere indicarle la tarea expl√≠citamente (ej. "summarize: ...") y es vers√°til, pero no est√° afinado espec√≠ficamente para resumen.
"""

from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch
from tqdm import tqdm
import evaluate

# Configuraci√≥n de dispositivo
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Cargar modelo y tokenizador T5
model_name = "t5-base"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)
model.eval()

# Cargar conjunto de test
test_input_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.src"
test_target_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.tgt.tagged"

with open(test_input_path, "r", encoding="utf-8") as f:
    test_inputs = [line.strip() for line in f.readlines()]

with open(test_target_path, "r", encoding="utf-8") as f:
    test_targets = [line.strip() for line in f.readlines()]

# Asegurarse de que est√©n alineados
assert len(test_inputs) == len(test_targets), "Las longitudes no coinciden"

# Limitar n√∫mero de ejemplos si es necesario
max_examples = 300
test_inputs = test_inputs[:max_examples]
test_targets = test_targets[:max_examples]

# Generar res√∫menes
generated_summaries = []

print("Generando res√∫menes...")
with torch.no_grad():
    for text in tqdm(test_inputs):
        input_text = f"summarize: {text}"  # prefijo requerido por T5
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            truncation=True,
            padding="max_length",
            max_length=512
        ).to(device)

        summary_ids = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=150,
            num_beams=4,
            early_stopping=True
        )

        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        generated_summaries.append(summary)

print("Res√∫menes generados.")

# Evaluar con ROUGE
print("Calculando m√©tricas ROUGE...")
rouge = evaluate.load("rouge")
results = rouge.compute(
    predictions=generated_summaries,
    references=test_targets,
    use_stemmer=True
)

# Redondear resultados
results = {k.upper(): round(v * 100, 2) for k, v in results.items()}

print("\nM√âTRICAS ROUGE ‚Äì T5 (TEST ‚Äì 300 ejemplos):")
for k, v in results.items():
    print(f"{k}: {v}")

from evaluate import load
import pandas as pd

# Cargar m√©trica ROUGE
rouge = load("rouge")

# Calcular ROUGE-L individual para cada ejemplo
scores = []
for pred, ref in zip(generated_summaries, test_targets):
    result = rouge.compute(predictions=[pred], references=[ref], use_stemmer=True)
    rouge_l = result["rougeL"]
    scores.append(rouge_l)

# Crear dataframe con todos los datos
df = pd.DataFrame({
    "input": test_inputs,
    "generated_summary": generated_summaries,
    "reference_summary": test_targets,
    "rougeL": scores
})

# Ordenar por puntuaci√≥n ROUGE-L
df_sorted = df.sort_values(by="rougeL", ascending=False)

# Mostrar los 5 mejores
print("\nEJEMPLOS CON MEJOR ROUGE-L:")
for i, row in df_sorted.head(5).iterrows():
    print(f"\nEjemplo con ROUGE-L: {round(row['rougeL'] * 100, 2)}%")
    print("Texto original:")
    print(row['input'])
    print("\nResumen generado:")
    print(row['generated_summary'])
    print("\nReferencia esperada:")
    print(row['reference_summary'])
    print("="*100)

# Mostrar los 5 peores
print("\nEJEMPLOS CON PEOR ROUGE-L:")
for i, row in df_sorted.tail(5).iterrows():
    print(f"\nEjemplo con ROUGE-L: {round(row['rougeL'] * 100, 2)}%")
    print("Texto original:")
    print(row['input'])
    print("\nResumen generado:")
    print(row['generated_summary'])
    print("\nReferencia esperada:")
    print(row['reference_summary'])
    print("="*100)

"""# t5-large

Versi√≥n m√°s grande del T5 con 770M par√°metros. Ofrece mejores resultados que t5-base en la mayor√≠a de tareas, manteniendo el mismo enfoque multitarea, pero con mayor costo computacional.
"""

from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch
from tqdm import tqdm
import evaluate

# Configuraci√≥n de dispositivo
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Cargar modelo y tokenizador T5-LARGE
model_name = "t5-large"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)
model.eval()

# Cargar datos
test_input_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.src"
test_target_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.tgt.tagged"

with open(test_input_path, "r", encoding="utf-8") as f:
    test_inputs = [line.strip() for line in f.readlines()]

with open(test_target_path, "r", encoding="utf-8") as f:
    test_targets = [line.strip() for line in f.readlines()]

# Validar longitud
assert len(test_inputs) == len(test_targets), "Las longitudes no coinciden"

# Limitar datos
max_examples = 300
test_inputs = test_inputs[:max_examples]
test_targets = test_targets[:max_examples]

# Generar res√∫menes
generated_summaries = []

print("Generando res√∫menes con T5-LARGE...")
with torch.no_grad():
    for text in tqdm(test_inputs):
        input_text = f"summarize: {text}"
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            truncation=True,
            padding="max_length",
            max_length=512
        ).to(device)

        summary_ids = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=192,
            num_beams=4,
            early_stopping=True
        )

        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        generated_summaries.append(summary)

print("Res√∫menes generados.")

# Evaluar con ROUGE
print("Calculando m√©tricas ROUGE...")
rouge = evaluate.load("rouge")
results = rouge.compute(
    predictions=generated_summaries,
    references=test_targets,
    use_stemmer=True
)

results = {k.upper(): round(v * 100, 2) for k, v in results.items()}

print("\nM√âTRICAS ROUGE ‚Äì T5-LARGE (TEST ‚Äì 300 ejemplos):")
for k, v in results.items():
    print(f"{k}: {v}")

from evaluate import load
import pandas as pd

# Cargar m√©trica ROUGE
rouge = load("rouge")

# Calcular ROUGE-L individual para cada ejemplo
scores = []
for pred, ref in zip(generated_summaries, test_targets):
    result = rouge.compute(predictions=[pred], references=[ref], use_stemmer=True)
    rouge_l = result["rougeL"]
    scores.append(rouge_l)

# Crear dataframe con todos los datos
df = pd.DataFrame({
    "input": test_inputs,
    "generated_summary": generated_summaries,
    "reference_summary": test_targets,
    "rougeL": scores
})

# Ordenar por puntuaci√≥n ROUGE-L
df_sorted = df.sort_values(by="rougeL", ascending=False)

# Mostrar los 5 mejores
print("\nEJEMPLOS CON MEJOR ROUGE-L:")
for i, row in df_sorted.head(5).iterrows():
    print(f"\nEjemplo con ROUGE-L: {round(row['rougeL'] * 100, 2)}%")
    print("Texto original:")
    print(row['input'])
    print("\nResumen generado:")
    print(row['generated_summary'])
    print("\nReferencia esperada:")
    print(row['reference_summary'])
    print("="*100)

# Mostrar los 5 peores
print("\nEJEMPLOS CON PEOR ROUGE-L:")
for i, row in df_sorted.tail(5).iterrows():
    print(f"\nEjemplo con ROUGE-L: {round(row['rougeL'] * 100, 2)}%")
    print("Texto original:")
    print(row['input'])
    print("\nResumen generado:")
    print(row['generated_summary'])
    print("\nReferencia esperada:")
    print(row['reference_summary'])
    print("="*100)

# N√∫mero de ejemplos a mostrar
num_examples = 5

print(f"\nMostrando {num_examples} ejemplos del conjunto de test:\n")
for i in range(num_examples):
    print(f"Ejemplo {i+1}")
    print("Texto original:")
    print(test_inputs[i])
    print("\nResumen generado:")
    print(generated_summaries[i])
    print("\nReferencia esperada:")
    print(test_targets[i])
    print("="*80)

"""# pegasus-cnn_dailymail"""

from transformers import PegasusTokenizer, PegasusForConditionalGeneration
import torch
import evaluate
from tqdm import tqdm

# Configuraci√≥n del dispositivo
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Cargar modelo y tokenizador Pegasus fine-tuned para resumen
model_name = "google/pegasus-cnn_dailymail"
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)
model.eval()

# Rutas del conjunto de test
test_input_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.src"
test_target_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.tgt.tagged"

# Cargar datos
with open(test_input_path, "r", encoding="utf-8") as f:
    test_inputs = [line.strip() for line in f.readlines()]

with open(test_target_path, "r", encoding="utf-8") as f:
    test_targets = [line.strip() for line in f.readlines()]

# Verificar alineaci√≥n
assert len(test_inputs) == len(test_targets), "Las longitudes no coinciden"

# Limitar n√∫mero de ejemplos si se desea
max_examples = 300
test_inputs = test_inputs[:max_examples]
test_targets = test_targets[:max_examples]

# Par√°metros
MAX_INPUT_LENGTH = 1024
MAX_OUTPUT_LENGTH = 192

# Generar res√∫menes
generated_summaries = []

print("üöÄ Generando res√∫menes con Pegasus...")
with torch.no_grad():
    for text in tqdm(test_inputs):
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding="max_length",
            max_length=MAX_INPUT_LENGTH
        ).to(device)

        summary_ids = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=MAX_OUTPUT_LENGTH,
            num_beams=4,
            early_stopping=True
        )

        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        generated_summaries.append(summary)

print("Res√∫menes generados.")

# Evaluaci√≥n con ROUGE
print("Calculando m√©tricas ROUGE...")
rouge = evaluate.load("rouge")
results = rouge.compute(
    predictions=generated_summaries,
    references=test_targets,
    use_stemmer=True
)

# Mostrar resultados redondeados
results = {k.upper(): round(v * 100, 2) for k, v in results.items()}

print("\nM√âTRICAS ROUGE ‚Äì Pegasus (TEST ‚Äì 300 ejemplos):")
for k, v in results.items():
    print(f"{k}: {v}")

from evaluate import load
import pandas as pd

# Cargar m√©trica ROUGE
rouge = load("rouge")

# Calcular ROUGE-L individual para cada ejemplo
scores = []
for pred, ref in zip(generated_summaries, test_targets):
    result = rouge.compute(predictions=[pred], references=[ref], use_stemmer=True)
    rouge_l = result["rougeL"]
    scores.append(rouge_l)

# Crear dataframe con todos los datos
df = pd.DataFrame({
    "input": test_inputs,
    "generated_summary": generated_summaries,
    "reference_summary": test_targets,
    "rougeL": scores
})

# Ordenar por puntuaci√≥n ROUGE-L
df_sorted = df.sort_values(by="rougeL", ascending=False)

# Mostrar los 5 mejores
print("\nEJEMPLOS CON MEJOR ROUGE-L:")
for i, row in df_sorted.head(5).iterrows():
    print(f"\nEjemplo con ROUGE-L: {round(row['rougeL'] * 100, 2)}%")
    print("Texto original:")
    print(row['input'])
    print("\nResumen generado:")
    print(row['generated_summary'])
    print("\nReferencia esperada:")
    print(row['reference_summary'])
    print("="*100)

# Mostrar los 5 peores
print("\nEJEMPLOS CON PEOR ROUGE-L:")
for i, row in df_sorted.tail(5).iterrows():
    print(f"\nEjemplo con ROUGE-L: {round(row['rougeL'] * 100, 2)}%")
    print("Texto original:")
    print(row['input'])
    print("\nResumen generado:")
    print(row['generated_summary'])
    print("\nReferencia esperada:")
    print(row['reference_summary'])
    print("="*100)

"""# pegasus-xsum"""

from transformers import PegasusTokenizer, PegasusForConditionalGeneration
import torch
import evaluate
from tqdm import tqdm

# Configurar dispositivo
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Cargar modelo Pegasus-XSum
model_name = "google/pegasus-xsum"
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)
model.eval()

# Cargar conjunto de test
test_input_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.src"
test_target_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.tgt.tagged"

with open(test_input_path, "r", encoding="utf-8") as f:
    test_inputs = [line.strip() for line in f.readlines()]

with open(test_target_path, "r", encoding="utf-8") as f:
    test_targets = [line.strip() for line in f.readlines()]

# Verificar alineaci√≥n
assert len(test_inputs) == len(test_targets), "Las longitudes no coinciden"

# Limitar n√∫mero de ejemplos si se desea
max_examples = 300
test_inputs = test_inputs[:max_examples]
test_targets = test_targets[:max_examples]

# Par√°metros de entrada/salida
MAX_INPUT_LENGTH = 512
MAX_OUTPUT_LENGTH = 64  # Pegasus-XSum produce res√∫menes m√°s cortos

# Generar res√∫menes
generated_summaries = []

print("Generando res√∫menes con Pegasus-XSum...")
with torch.no_grad():
    for text in tqdm(test_inputs):
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding="max_length",
            max_length=MAX_INPUT_LENGTH
        ).to(device)

        summary_ids = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=MAX_OUTPUT_LENGTH,
            num_beams=4,
            early_stopping=True
        )

        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        generated_summaries.append(summary)

print("Res√∫menes generados.")

# Evaluar con ROUGE
print("Calculando m√©tricas ROUGE...")
rouge = evaluate.load("rouge")
results = rouge.compute(
    predictions=generated_summaries,
    references=test_targets,
    use_stemmer=True
)

results = {k.upper(): round(v * 100, 2) for k, v in results.items()}

print("\nM√âTRICAS ROUGE ‚Äì Pegasus-XSum (TEST ‚Äì 300 ejemplos):")
for k, v in results.items():
    print(f"{k}: {v}")

from evaluate import load
import pandas as pd

# Cargar m√©trica ROUGE
rouge = load("rouge")

# Calcular ROUGE-L individual para cada ejemplo
scores = []
for pred, ref in zip(generated_summaries, test_targets):
    result = rouge.compute(predictions=[pred], references=[ref], use_stemmer=True)
    rouge_l = result["rougeL"]
    scores.append(rouge_l)

# Crear dataframe con todos los datos
df = pd.DataFrame({
    "input": test_inputs,
    "generated_summary": generated_summaries,
    "reference_summary": test_targets,
    "rougeL": scores
})

# Ordenar por puntuaci√≥n ROUGE-L
df_sorted = df.sort_values(by="rougeL", ascending=False)

# Mostrar los 5 mejores
print("\nEJEMPLOS CON MEJOR ROUGE-L:")
for i, row in df_sorted.head(5).iterrows():
    print(f"\nEjemplo con ROUGE-L: {round(row['rougeL'] * 100, 2)}%")
    print("Texto original:")
    print(row['input'])
    print("\nResumen generado:")
    print(row['generated_summary'])
    print("\nReferencia esperada:")
    print(row['reference_summary'])
    print("="*100)

# Mostrar los 5 peores
print("\nEJEMPLOS CON PEOR ROUGE-L:")
for i, row in df_sorted.tail(5).iterrows():
    print(f"\nEjemplo con ROUGE-L: {round(row['rougeL'] * 100, 2)}%")
    print("Texto original:")
    print(row['input'])
    print("\nResumen generado:")
    print(row['generated_summary'])
    print("\nReferencia esperada:")
    print(row['reference_summary'])
    print("="*100)

"""# pegasus-pubmed"""

from transformers import PegasusTokenizer, PegasusForConditionalGeneration
import torch
import evaluate
from tqdm import tqdm

# Configurar dispositivo
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Cargar modelo Pegasus-XSum
model_name = "google/pegasus-pubmed"
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)
model.eval()

# Cargar conjunto de test
test_input_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.src"
test_target_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.tgt.tagged"

with open(test_input_path, "r", encoding="utf-8") as f:
    test_inputs = [line.strip() for line in f.readlines()]

with open(test_target_path, "r", encoding="utf-8") as f:
    test_targets = [line.strip() for line in f.readlines()]

# Verificar alineaci√≥n
assert len(test_inputs) == len(test_targets), "Las longitudes no coinciden"

# Limitar n√∫mero de ejemplos si se desea
max_examples = 300
test_inputs = test_inputs[:max_examples]
test_targets = test_targets[:max_examples]

# Par√°metros de entrada/salida
MAX_INPUT_LENGTH = 512
MAX_OUTPUT_LENGTH = 64  # Pegasus-XSum produce res√∫menes m√°s cortos

# Generar res√∫menes
generated_summaries = []

print("Generando res√∫menes con Pegasus-XSum...")
with torch.no_grad():
    for text in tqdm(test_inputs):
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding="max_length",
            max_length=MAX_INPUT_LENGTH
        ).to(device)

        summary_ids = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=MAX_OUTPUT_LENGTH,
            num_beams=4,
            early_stopping=True
        )

        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        generated_summaries.append(summary)

print("Res√∫menes generados.")

# Evaluar con ROUGE
print("Calculando m√©tricas ROUGE...")
rouge = evaluate.load("rouge")
results = rouge.compute(
    predictions=generated_summaries,
    references=test_targets,
    use_stemmer=True
)

results = {k.upper(): round(v * 100, 2) for k, v in results.items()}

print("\nM√âTRICAS ROUGE ‚Äì Pegasus-XSum (TEST ‚Äì 300 ejemplos):")
for k, v in results.items():
    print(f"{k}: {v}")

from evaluate import load
import pandas as pd

# Cargar m√©trica ROUGE
rouge = load("rouge")

# Calcular ROUGE-L individual para cada ejemplo
scores = []
for pred, ref in zip(generated_summaries, test_targets):
    result = rouge.compute(predictions=[pred], references=[ref], use_stemmer=True)
    rouge_l = result["rougeL"]
    scores.append(rouge_l)

# Crear dataframe con todos los datos
df = pd.DataFrame({
    "input": test_inputs,
    "generated_summary": generated_summaries,
    "reference_summary": test_targets,
    "rougeL": scores
})

# Ordenar por puntuaci√≥n ROUGE-L
df_sorted = df.sort_values(by="rougeL", ascending=False)

# Mostrar los 5 mejores
print("\nEJEMPLOS CON MEJOR ROUGE-L:")
for i, row in df_sorted.head(5).iterrows():
    print(f"\nEjemplo con ROUGE-L: {round(row['rougeL'] * 100, 2)}%")
    print("Texto original:")
    print(row['input'])
    print("\nResumen generado:")
    print(row['generated_summary'])
    print("\nReferencia esperada:")
    print(row['reference_summary'])
    print("="*100)

# Mostrar los 5 peores
print("\nEJEMPLOS CON PEOR ROUGE-L:")
for i, row in df_sorted.tail(5).iterrows():
    print(f"\nEjemplo con ROUGE-L: {round(row['rougeL'] * 100, 2)}%")
    print("Texto original:")
    print(row['input'])
    print("\nResumen generado:")
    print(row['generated_summary'])
    print("\nReferencia esperada:")
    print(row['reference_summary'])
    print("="*100)

"""# facebook/bart-large-cnn

Modelo BART fine-tuned espec√≠ficamente para resumen de art√≠culos de noticias (CNN/DailyMail). Cuenta con 406M par√°metros y produce res√∫menes coherentes y naturales sin necesidad de prefijos. Ideal para tareas de summarization directamente.
"""

from transformers import BartTokenizer, BartForConditionalGeneration
import torch
import evaluate
from tqdm import tqdm

# Configuraci√≥n del dispositivo
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Cargar modelo y tokenizador
model_name = "facebook/bart-large-cnn"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name).to(device)
model.eval()

# Cargar datos
test_input_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.src"
test_target_path = "/content/drive/MyDrive/TFG/Dataset/test.txt.tgt.tagged"

with open(test_input_path, "r", encoding="utf-8") as f:
    test_inputs = [line.strip() for line in f.readlines()]

with open(test_target_path, "r", encoding="utf-8") as f:
    test_targets = [line.strip() for line in f.readlines()]

# Verificar alineaci√≥n
assert len(test_inputs) == len(test_targets), "Las longitudes no coinciden"

# Limitar por memoria si se desea
max_examples = 300
test_inputs = test_inputs[:max_examples]
test_targets = test_targets[:max_examples]

# Par√°metros del modelo
MAX_INPUT_LENGTH = 1024
MAX_OUTPUT_LENGTH = 192

# Generaci√≥n de res√∫menes
generated_summaries = []

print("Generando res√∫menes con BART...")
with torch.no_grad():
    for text in tqdm(test_inputs):
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding="max_length",
            max_length=MAX_INPUT_LENGTH
        ).to(device)

        summary_ids = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=MAX_OUTPUT_LENGTH,
            num_beams=4,
            early_stopping=True
        )

        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        generated_summaries.append(summary)

print("Res√∫menes generados.")

# Evaluaci√≥n ROUGE
print("Calculando m√©tricas ROUGE...")
rouge = evaluate.load("rouge")

results = rouge.compute(
    predictions=generated_summaries,
    references=test_targets,
    use_stemmer=True
)

# Mostrar resultados redondeados
results = {k.upper(): round(v * 100, 2) for k, v in results.items()}

print("\nM√âTRICAS ROUGE ‚Äì BART (TEST ‚Äì 300 ejemplos):")
for k, v in results.items():
    print(f"{k}: {v}")

# N√∫mero de ejemplos a mostrar
num_examples = 5

print(f"\nMostrando {num_examples} ejemplos del conjunto de test:\n")
for i in range(num_examples):
    print(f"Ejemplo {i+1}")
    print("Texto original:")
    print(test_inputs[i])
    print("\nResumen generado:")
    print(generated_summaries[i])
    print("\nReferencia esperada:")
    print(test_targets[i])
    print("="*80)

from evaluate import load
import pandas as pd

# Cargar m√©trica ROUGE
rouge = load("rouge")

# Calcular ROUGE-L individual para cada ejemplo
scores = []
for pred, ref in zip(generated_summaries, test_targets):
    result = rouge.compute(predictions=[pred], references=[ref], use_stemmer=True)
    rouge_l = result["rougeL"]
    scores.append(rouge_l)

# Crear dataframe con todos los datos
df = pd.DataFrame({
    "input": test_inputs,
    "generated_summary": generated_summaries,
    "reference_summary": test_targets,
    "rougeL": scores
})

# Ordenar por puntuaci√≥n ROUGE-L
df_sorted = df.sort_values(by="rougeL", ascending=False)

# Mostrar los 5 mejores
print("\nEJEMPLOS CON MEJOR ROUGE-L:")
for i, row in df_sorted.head(5).iterrows():
    print(f"\nüîπ Ejemplo con ROUGE-L: {round(row['rougeL'] * 100, 2)}%")
    print("üì• Texto original:")
    print(row['input'])
    print("\nüìù Resumen generado:")
    print(row['generated_summary'])
    print("\nüéØ Referencia esperada:")
    print(row['reference_summary'])
    print("="*100)

# Mostrar los 5 peores
print("\nEJEMPLOS CON PEOR ROUGE-L:")
for i, row in df_sorted.tail(5).iterrows():
    print(f"\nEjemplo con ROUGE-L: {round(row['rougeL'] * 100, 2)}%")
    print("Texto original:")
    print(row['input'])
    print("\nResumen generado:")
    print(row['generated_summary'])
    print("\nReferencia esperada:")
    print(row['reference_summary'])
    print("="*100)